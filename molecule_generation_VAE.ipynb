{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!pip -q install rdkit-pypi==2021.9.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import ast\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem import BondType\n",
    "from rdkit.Chem.Draw import MolsToGridImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "RDLogger.DisableLog(\"rdApp.*\")\n",
    "\n",
    "csv_path = keras.utils.get_file(\n",
    "    \"250k_rndm_zinc_drugs_clean_3.csv\",\n",
    "    \"https://raw.githubusercontent.com/aspuru-guzik-group/chemical_vae/master/models/zinc_properties/250k_rndm_zinc_drugs_clean_3.csv\",\n",
    ")\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "df[\"smiles\"] = df[\"smiles\"].apply(lambda s: s.replace(\"\\n\", \"\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "---\n",
    "## Hyperparameters\n",
    "SMILE_CHARSET = '[\"C\", \"B\", \"F\", \"I\", \"H\", \"O\", \"N\", \"S\", \"P\", \"Cl\", \"Br\"]'\n",
    "\n",
    "bond_mapping = {\"SINGLE\": 0, \"DOUBLE\": 1, \"TRIPLE\": 2, \"AROMATIC\": 3}\n",
    "bond_mapping.update(\n",
    "    {0: BondType.SINGLE, 1: BondType.DOUBLE, 2: BondType.TRIPLE, 3: BondType.AROMATIC}\n",
    ")\n",
    "SMILE_CHARSET = ast.literal_eval(SMILE_CHARSET)\n",
    "\n",
    "MAX_MOLSIZE = max(df[\"smiles\"].str.len())\n",
    "SMILE_to_index = dict((c, i) for i, c in enumerate(SMILE_CHARSET))\n",
    "index_to_SMILE = dict((i, c) for i, c in enumerate(SMILE_CHARSET))\n",
    "atom_mapping = dict(SMILE_to_index)\n",
    "atom_mapping.update(index_to_SMILE)\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 10\n",
    "\n",
    "VAE_LR = 5e-4\n",
    "NUM_ATOMS = 120  # Maximum number of atoms\n",
    "\n",
    "ATOM_DIM = len(SMILE_CHARSET)  # Number of atom types\n",
    "BOND_DIM = 4 + 1  # Number of bond types\n",
    "LATENT_DIM = 435  # Size of the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def smiles_to_graph(smiles):\n",
    "    molecule = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "    # Initialize adjacency and feature tensor\n",
    "    adjacency = np.zeros((BOND_DIM, NUM_ATOMS, NUM_ATOMS), \"float32\")\n",
    "    features = np.zeros((NUM_ATOMS, ATOM_DIM), \"float32\")\n",
    "\n",
    "    for atom in molecule.GetAtoms():\n",
    "        i = atom.GetIdx()\n",
    "        atom_type = atom_mapping[atom.GetSymbol()]\n",
    "        features[i] = np.eye(ATOM_DIM)[atom_type]\n",
    "        # loop over one-hop neighbors\n",
    "        for neighbor in atom.GetNeighbors():\n",
    "            j = neighbor.GetIdx()\n",
    "            bond = molecule.GetBondBetweenAtoms(i, j)\n",
    "            bond_type_idx = bond_mapping[bond.GetBondType().name]\n",
    "            adjacency[bond_type_idx, [i, j], [j, i]] = 1\n",
    "\n",
    "\n",
    "    adjacency[-1, np.sum(adjacency, axis=0) == 0] = 1\n",
    "\n",
    "    features[np.where(np.sum(features, axis=1) == 0)[0], -1] = 1\n",
    "\n",
    "    return adjacency, features\n",
    "\n",
    "\n",
    "def graph_to_molecule(graph):\n",
    "    adjacency, features = graph\n",
    "\n",
    "    molecule = Chem.RWMol()\n",
    "\n",
    "    keep_idx = np.where(\n",
    "        (np.argmax(features, axis=1) != ATOM_DIM - 1)\n",
    "        & (np.sum(adjacency[:-1], axis=(0, 1)) != 0)\n",
    "    )[0]\n",
    "    features = features[keep_idx]\n",
    "    adjacency = adjacency[:, keep_idx, :][:, :, keep_idx]\n",
    "\n",
    "    for atom_type_idx in np.argmax(features, axis=1):\n",
    "        atom = Chem.Atom(atom_mapping[atom_type_idx])\n",
    "        _ = molecule.AddAtom(atom)\n",
    "\n",
    "\n",
    "    (bonds_ij, atoms_i, atoms_j) = np.where(np.triu(adjacency) == 1)\n",
    "    for bond_ij, atom_i, atom_j in zip(bonds_ij, atoms_i, atoms_j):\n",
    "        if atom_i == atom_j or bond_ij == BOND_DIM - 1:\n",
    "            continue\n",
    "        bond_type = bond_mapping[bond_ij]\n",
    "        molecule.AddBond(int(atom_i), int(atom_j), bond_type)\n",
    "\n",
    "    # Sanitize the molecule; for more information on sanitization, see\n",
    "    # https://www.rdkit.org/docs/RDKit_Book.html#molecular-sanitization\n",
    "    flag = Chem.SanitizeMol(molecule, catchErrors=True)\n",
    "    if flag != Chem.SanitizeFlags.SANITIZE_NONE:\n",
    "        return None\n",
    "\n",
    "    return molecule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_df = df.sample(frac=0.75, random_state=42)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "adjacency_tensor, feature_tensor, qed_tensor = [], [], []\n",
    "for idx in range(8000):\n",
    "    adjacency, features = smiles_to_graph(train_df.loc[idx][\"smiles\"])\n",
    "    qed = train_df.loc[idx][\"qed\"]\n",
    "    adjacency_tensor.append(adjacency)\n",
    "    feature_tensor.append(features)\n",
    "    qed_tensor.append(qed)\n",
    "\n",
    "adjacency_tensor = np.array(adjacency_tensor)\n",
    "feature_tensor = np.array(feature_tensor)\n",
    "qed_tensor = np.array(qed_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class RelationalGraphConvLayer(keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        units=128,\n",
    "        activation=\"relu\",\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        bias_initializer=\"zeros\",\n",
    "        kernel_regularizer=None,\n",
    "        bias_regularizer=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = keras.initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = keras.regularizers.get(bias_regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        bond_dim = input_shape[0][1]\n",
    "        atom_dim = input_shape[1][2]\n",
    "\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(bond_dim, atom_dim, self.units),\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            trainable=True,\n",
    "            name=\"W\",\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(\n",
    "                shape=(bond_dim, 1, self.units),\n",
    "                initializer=self.bias_initializer,\n",
    "                regularizer=self.bias_regularizer,\n",
    "                trainable=True,\n",
    "                name=\"b\",\n",
    "                dtype=\"float32\",\n",
    "            )\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        adjacency, features = inputs\n",
    "        x = ops.matmul(adjacency, features[:, None])\n",
    "        x = ops.matmul(x, self.kernel)\n",
    "        if self.use_bias:\n",
    "            x += self.bias\n",
    "        x_reduced = ops.sum(x, axis=1)\n",
    "        return self.activation(x_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_encoder(\n",
    "    gconv_units, latent_dim, adjacency_shape, feature_shape, dense_units, dropout_rate\n",
    "):\n",
    "    adjacency = layers.Input(shape=adjacency_shape)\n",
    "    features = layers.Input(shape=feature_shape)\n",
    "\n",
    "    features_transformed = features\n",
    "    for units in gconv_units:\n",
    "        features_transformed = RelationalGraphConvLayer(units)(\n",
    "            [adjacency, features_transformed]\n",
    "        )\n",
    "    x = layers.GlobalAveragePooling1D()(features_transformed)\n",
    "\n",
    "    for units in dense_units:\n",
    "        x = layers.Dense(units, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    z_mean = layers.Dense(latent_dim, dtype=\"float32\", name=\"z_mean\")(x)\n",
    "    log_var = layers.Dense(latent_dim, dtype=\"float32\", name=\"log_var\")(x)\n",
    "\n",
    "    encoder = keras.Model([adjacency, features], [z_mean, log_var], name=\"encoder\")\n",
    "\n",
    "    return encoder\n",
    "\n",
    "def get_decoder(dense_units, dropout_rate, latent_dim, adjacency_shape, feature_shape):\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "\n",
    "    x = latent_inputs\n",
    "    for units in dense_units:\n",
    "        x = layers.Dense(units, activation=\"tanh\")(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    x_adjacency = layers.Dense(np.prod(adjacency_shape))(x)\n",
    "    x_adjacency = layers.Reshape(adjacency_shape)(x_adjacency)\n",
    "    # Symmetrify tensors in the last two dimensions\n",
    "    x_adjacency = (x_adjacency + ops.transpose(x_adjacency, (0, 1, 3, 2))) / 2\n",
    "    x_adjacency = layers.Softmax(axis=1)(x_adjacency)\n",
    "\n",
    "    # Map outputs of previous layer (x) to [continuous] feature tensors (x_features)\n",
    "    x_features = layers.Dense(np.prod(feature_shape))(x)\n",
    "    x_features = layers.Reshape(feature_shape)(x_features)\n",
    "    x_features = layers.Softmax(axis=2)(x_features)\n",
    "\n",
    "    decoder = keras.Model(\n",
    "        latent_inputs, outputs=[x_adjacency, x_features], name=\"decoder\"\n",
    "    )\n",
    "\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    def __init__(self, seed=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.seed_generator = keras.random.SeedGenerator(seed)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch, dim = ops.shape(z_log_var)\n",
    "        epsilon = keras.random.normal(shape=(batch, dim), seed=self.seed_generator)\n",
    "        return z_mean + ops.exp(0.5 * z_log_var) * epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MoleculeGenerator(keras.Model):\n",
    "    def __init__(self, encoder, decoder, max_len, seed=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.property_prediction_layer = layers.Dense(1)\n",
    "        self.max_len = max_len\n",
    "        self.seed_generator = keras.random.SeedGenerator(seed)\n",
    "        self.sampling_layer = Sampling(seed=seed)\n",
    "\n",
    "        self.train_total_loss_tracker = keras.metrics.Mean(name=\"train_total_loss\")\n",
    "        self.val_total_loss_tracker = keras.metrics.Mean(name=\"val_total_loss\")\n",
    "        \n",
    "    def train_step(self, data):\n",
    "        adjacency_tensor, feature_tensor, qed_tensor = data[0]\n",
    "        graph_real = [adjacency_tensor, feature_tensor]\n",
    "        self.batch_size = ops.shape(qed_tensor)[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, qed_pred, gen_adjacency, gen_features = self(\n",
    "                graph_real, training=True\n",
    "            )\n",
    "            graph_generated = [gen_adjacency, gen_features]\n",
    "            total_loss = self._compute_loss(\n",
    "                z_log_var, z_mean, qed_tensor, qed_pred, graph_real, graph_generated\n",
    "            )\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        self.train_total_loss_tracker.update_state(total_loss)\n",
    "        return {\"loss\": self.train_total_loss_tracker.result()}\n",
    "    \n",
    "    def _compute_loss(\n",
    "        self, z_log_var, z_mean, qed_true, qed_pred, graph_real, graph_generated\n",
    "    ):\n",
    "        adjacency_real, features_real = graph_real\n",
    "        adjacency_gen, features_gen = graph_generated\n",
    "\n",
    "        adjacency_loss = ops.mean(\n",
    "            ops.sum(\n",
    "                keras.losses.categorical_crossentropy(\n",
    "                    adjacency_real, adjacency_gen, axis=1\n",
    "                ),\n",
    "                axis=(1, 2),\n",
    "            )\n",
    "        )\n",
    "        features_loss = ops.mean(\n",
    "            ops.sum(\n",
    "                keras.losses.categorical_crossentropy(features_real, features_gen),\n",
    "                axis=(1),\n",
    "            )\n",
    "        )\n",
    "        kl_loss = -0.5 * ops.sum(\n",
    "            1 + z_log_var - z_mean**2 - ops.minimum(ops.exp(z_log_var), 1e6), 1\n",
    "        )\n",
    "        kl_loss = ops.mean(kl_loss)\n",
    "\n",
    "        property_loss = ops.mean(\n",
    "            keras.losses.binary_crossentropy(qed_true, ops.squeeze(qed_pred, axis=1))\n",
    "        )\n",
    "\n",
    "        graph_loss = self._gradient_penalty(graph_real, graph_generated)\n",
    "\n",
    "        return kl_loss + property_loss + graph_loss + adjacency_loss + features_loss\n",
    "    \n",
    "    def _gradient_penalty(self, graph_real, graph_generated):\n",
    "        # Unpack graphs\n",
    "        adjacency_real, features_real = graph_real\n",
    "        adjacency_generated, features_generated = graph_generated\n",
    "\n",
    "        # Generate interpolated graphs (adjacency_interp and features_interp)\n",
    "        alpha = keras.random.uniform(shape=(self.batch_size,), seed=self.seed_generator)\n",
    "        alpha = ops.reshape(alpha, (self.batch_size, 1, 1, 1))\n",
    "        adjacency_interp = (adjacency_real * alpha) + (\n",
    "            1.0 - alpha\n",
    "        ) * adjacency_generated\n",
    "        alpha = ops.reshape(alpha, (self.batch_size, 1, 1))\n",
    "        features_interp = (features_real * alpha) + (1.0 - alpha) * features_generated\n",
    "\n",
    "        # Compute the logits of interpolated graphs\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(adjacency_interp)\n",
    "            tape.watch(features_interp)\n",
    "            _, _, logits, _, _ = self(\n",
    "                [adjacency_interp, features_interp], training=True\n",
    "            )\n",
    "\n",
    "        # Compute the gradients with respect to the interpolated graphs\n",
    "        grads = tape.gradient(logits, [adjacency_interp, features_interp])\n",
    "        # Compute the gradient penalty\n",
    "        grads_adjacency_penalty = (1 - ops.norm(grads[0], axis=1)) ** 2\n",
    "        grads_features_penalty = (1 - ops.norm(grads[1], axis=2)) ** 2\n",
    "        return ops.mean(\n",
    "            ops.mean(grads_adjacency_penalty, axis=(-2, -1))\n",
    "            + ops.mean(grads_features_penalty, axis=(-1))\n",
    "        )\n",
    "        \n",
    "    def inference(self, batch_size):\n",
    "        z = keras.random.normal(\n",
    "            shape=(batch_size, LATENT_DIM), seed=self.seed_generator\n",
    "        )\n",
    "        reconstruction_adjacency, reconstruction_features = model.decoder.predict(z)\n",
    "        # obtain one-hot encoded adjacency tensor\n",
    "        adjacency = ops.argmax(reconstruction_adjacency, axis=1)\n",
    "        adjacency = ops.one_hot(adjacency, num_classes=BOND_DIM, axis=1)\n",
    "        # Remove potential self-loops from adjacency\n",
    "        adjacency = adjacency * (1.0 - ops.eye(NUM_ATOMS, dtype=\"float32\")[None, None])\n",
    "        # obtain one-hot encoded feature tensor\n",
    "        features = ops.argmax(reconstruction_features, axis=2)\n",
    "        features = ops.one_hot(features, num_classes=ATOM_DIM, axis=2)\n",
    "        return [\n",
    "            graph_to_molecule([adjacency[i].numpy(), features[i].numpy()])\n",
    "            for i in range(batch_size)\n",
    "        ]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, log_var = self.encoder(inputs)\n",
    "        z = self.sampling_layer([z_mean, log_var])\n",
    "\n",
    "        gen_adjacency, gen_features = self.decoder(z)\n",
    "\n",
    "        property_pred = self.property_prediction_layer(z_mean)\n",
    "\n",
    "        return z_mean, log_var, property_pred, gen_adjacency, gen_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "vae_optimizer = keras.optimizers.Adam(learning_rate=VAE_LR)\n",
    "\n",
    "encoder = get_encoder(\n",
    "    gconv_units=[9],\n",
    "    adjacency_shape=(BOND_DIM, NUM_ATOMS, NUM_ATOMS),\n",
    "    feature_shape=(NUM_ATOMS, ATOM_DIM),\n",
    "    latent_dim=LATENT_DIM,\n",
    "    dense_units=[512],\n",
    "    dropout_rate=0.0,\n",
    ")\n",
    "decoder = get_decoder(\n",
    "    dense_units=[128, 256, 512],\n",
    "    dropout_rate=0.2,\n",
    "    latent_dim=LATENT_DIM,\n",
    "    adjacency_shape=(BOND_DIM, NUM_ATOMS, NUM_ATOMS),\n",
    "    feature_shape=(NUM_ATOMS, ATOM_DIM),\n",
    ")\n",
    "\n",
    "model = MoleculeGenerator(encoder, decoder, MAX_MOLSIZE)\n",
    "\n",
    "model.compile(vae_optimizer)\n",
    "history = model.fit([adjacency_tensor, feature_tensor, qed_tensor], epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "molecules = model.inference(1000)\n",
    "\n",
    "MolsToGridImage(\n",
    "    [m for m in molecules if m is not None][:1000], molsPerRow=5, subImgSize=(260, 160)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def plot_latent(vae, data, labels):\n",
    "    # display a 2D plot of the property in the latent space\n",
    "    z_mean, _ = vae.encoder.predict(data)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_latent(model, [adjacency_tensor[:8000], feature_tensor[:8000]], qed_tensor[:8000])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
